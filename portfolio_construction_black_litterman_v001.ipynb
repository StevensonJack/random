{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackstevenson/CQF/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# database\n",
    "import yfinance as yf\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# quadratic\n",
    "import quadprog\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# visualisation\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# maybe\n",
    "import plotly.express as px\n",
    "import matplotlib\n",
    "from matplotlib.patches import Patch\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.style.use('fivethirtyeight')\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/lz8knlfn5ys693qpn6bnjb440000gn/T/ipykernel_36581/3435988170.py:21: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  simple_returns = df.drop(columns=['Factor SMB', 'Factor HML', 'Factor RMW', 'Factor CMA']).pct_change()\n"
     ]
    }
   ],
   "source": [
    "# create our engine\n",
    "engine = create_engine(\"sqlite:///project_portfolio.db\")\n",
    "\n",
    "df = pd.read_sql('portfolio_data', con=engine)\n",
    "\n",
    "# # set date as index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# create separate t bill dataframe\n",
    "three_month_tbill = df['3M TB'] / 252\n",
    "\n",
    "# create separate benchmark dataframe\n",
    "benchmark_spx_500 = df['Benchmark - S&P 500'].pct_change()\n",
    "\n",
    "# consistent ordering\n",
    "df = df[['Energy', 'Materials', 'Industrials', 'Consumer Discretionary', 'Consumer Staples', 'Health Care', 'Financials', 'Information Technology', \n",
    "         'Communication Services', 'Utilities', 'Real Estate', 'Volatility (Exo)', 'Commodities (Exo)', 'Bonds (Exo)', 'Factor SMB', 'Factor HML', \n",
    "         'Factor RMW', 'Factor CMA']]\n",
    "\n",
    "# calculate simple returns\n",
    "simple_returns = df.drop(columns=['Factor SMB', 'Factor HML', 'Factor RMW', 'Factor CMA']).pct_change()\n",
    "\n",
    "simple_returns = pd.concat([simple_returns, df[['Factor SMB', 'Factor HML', 'Factor RMW', 'Factor CMA']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Factor - Momentum Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookback period 252 (approximate 12 months)\n",
    "lookback_period = 252\n",
    "\n",
    "# exclude the exogenous and fama-french\n",
    "sector_momentum = simple_returns.drop(columns=['Volatility (Exo)', 'Commodities (Exo)', \n",
    "                                               'Bonds (Exo)','Factor SMB', 'Factor HML', \n",
    "                                               'Factor RMW', 'Factor CMA']).rolling(window=lookback_period).apply(lambda x: (x + 1).prod() - 1)\n",
    "\n",
    "# rank the sector returns over lookback plus lag to prevent perfect portfolio\n",
    "momentum_ranks = sector_momentum.rank(axis=1, method='first', ascending=False).shift(1).dropna()\n",
    "\n",
    "# identify top and bottom\n",
    "top_momentum_sectors = momentum_ranks <= 3  # Top 3 sectors\n",
    "bottom_momentum_sectors = momentum_ranks >= 9  # Bottom 3 sectors\n",
    "\n",
    "# construct portfolio returns from top & bottom\n",
    "momentum_high_portfolio = simple_returns[top_momentum_sectors].mean(axis=1)\n",
    "momentum_low_portfolio = simple_returns[bottom_momentum_sectors].mean(axis=1)\n",
    "\n",
    "# calculate momentum factor returns\n",
    "momentum_factor_return = momentum_high_portfolio - momentum_low_portfolio\n",
    "momentum_factor_return_df = momentum_factor_return.to_frame(name='Factor MOM')\n",
    "\n",
    "# add into simple_returns_df\n",
    "simple_returns = pd.concat([simple_returns, momentum_factor_return_df], axis=1)\n",
    "\n",
    "# drop the first 12 months due to lookback\n",
    "simple_returns.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract the 3m t bill daily rate\n",
    "excess_returns = simple_returns.sub(three_month_tbill, axis=0)\n",
    "\n",
    "# calculate log returns returns\n",
    "log_returns = np.log(1 + simple_returns).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Black-Litterman Class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackLittermanModel:\n",
    "    def __init__(self, equilibrium_weights, log_returns, risk_aversion, views_mapping_matrix, views_matrix, tracking_error_target=0.00000009, tau=0.025):\n",
    "        self._equilibrium_weights = equilibrium_weights\n",
    "        self._log_returns = log_returns\n",
    "        self._risk_aversion = risk_aversion\n",
    "        self._tau = tau\n",
    "        self._covariance_matrix = self._calculate_covariance_matrix()\n",
    "        self._views_mapping_matrix = views_mapping_matrix\n",
    "        self._views_matrix = views_matrix\n",
    "        self._tracking_error_target = tracking_error_target \n",
    "\n",
    "    # TODO: private methods\n",
    "    def _calculate_covariance_matrix(self):\n",
    "        # calculate the vols\n",
    "        volatilities_array = self._log_returns.std()\n",
    "\n",
    "        # calculate correlation\n",
    "        correlation_coefficients = self._log_returns.corr()\n",
    "\n",
    "        # create the diagonal vol matrix (vol on the diagonal, zeros elsewhere)\n",
    "        std_diag_vol_matrix  = np.diag(volatilities_array)\n",
    "\n",
    "        # compute the covariance matrix\n",
    "        covariance_matrix = std_diag_vol_matrix @ correlation_coefficients.values @ std_diag_vol_matrix\n",
    "\n",
    "        # pass the headers from log return \n",
    "\n",
    "        covariance_matrix = pd.DataFrame(\n",
    "            covariance_matrix,\n",
    "            columns = self._log_returns.columns,\n",
    "            index = self._log_returns.columns,\n",
    "        )\n",
    "\n",
    "        return covariance_matrix\n",
    "    \n",
    "    def calculate_views_adjusted_returns(self):\n",
    "        # we need implied equity returns and omega\n",
    "        implied_returns_vector = self.calculate_implied_equilibrium_returns().values\n",
    "\n",
    "        omega = self.calculate_uncertainty_views_matrix()\n",
    "\n",
    "        # we have three terms for the np.dot product\n",
    "        tau_sigma_transpose_p = self._tau * (self._covariance_matrix.values @ self._views_mapping_matrix.T)\n",
    "\n",
    "        inverse_middle_term = np.linalg.inv((self._views_mapping_matrix @ tau_sigma_transpose_p) + omega)\n",
    "\n",
    "        view_minus_p = (self._views_matrix - (self._views_mapping_matrix @ implied_returns_vector))\n",
    "\n",
    "        views_adjusted_returns_vector = implied_returns_vector + ((tau_sigma_transpose_p @ inverse_middle_term) @ view_minus_p)\n",
    "\n",
    "        # convert to dataframe for consistency\n",
    "        views_adjusted_returns = pd.DataFrame(views_adjusted_returns_vector, columns=[\"Adjusted Return\"], index=self._log_returns.columns)\n",
    "\n",
    "        return views_adjusted_returns\n",
    "    \n",
    "    # calculate the covariance of the error terms aka omega\n",
    "    def calculate_uncertainty_views_matrix(self):\n",
    "        # you extract the diagonal and then convert to a diagonal matrix with zeros\n",
    "        omega = np.diag(((self._views_mapping_matrix @ (self._tau * self._covariance_matrix)) @ self._views_mapping_matrix.T))\n",
    "        \n",
    "        return np.diag(omega)\n",
    "\n",
    "    \n",
    "    def calculate_implied_equilibrium_returns(self):\n",
    "        implied_returns_vector = self._risk_aversion * (self._covariance_matrix @ self._equilibrium_weights)\n",
    "\n",
    "        implied_returns = pd.DataFrame(implied_returns_vector, columns=[\"Implied Return\"], index=self._log_returns.columns)\n",
    "\n",
    "        return implied_returns\n",
    "    \n",
    "    def calculate_unconstrained_mv_optimisation(self):\n",
    "        # invert the covariance matrix\n",
    "        inv_cov_matrix = np.linalg.inv(self._covariance_matrix)\n",
    "\n",
    "        # calculate adjusted returns\n",
    "        adjusted_returns = self.calculate_views_adjusted_returns().values\n",
    "\n",
    "        # obtain weights\n",
    "        optimal_weights = self._risk_aversion * np.dot(inv_cov_matrix, adjusted_returns)\n",
    "\n",
    "        # # normalise the sum of weights to 1\n",
    "        optimal_weights /= np.sum(optimal_weights)\n",
    "\n",
    "        # convert to dataframe for consistency\n",
    "        optimal_weights = pd.DataFrame(optimal_weights, columns=[\"Adjusted Weights\"], index=self._log_returns.columns)\n",
    "\n",
    "        return optimal_weights\n",
    "    \n",
    "    def generate_mv_constraint_matrices(self, implied_returns_vector_values):\n",
    "        # we will generate our two constraints - no short selling & no leverage\n",
    "        number_of_assets = len(implied_returns_vector_values)\n",
    "\n",
    "        # no short selling\n",
    "        no_short_selling_constraint_coefficient = np.eye(number_of_assets)\n",
    "        no_short_selling_constraint_rhs = np.zeros(number_of_assets)\n",
    "\n",
    "        # no leverage - you need to transpose the \n",
    "        no_leverage_constraint_coefficient = np.ones([1, number_of_assets])\n",
    "        no_leverage_constraint_rhs = np.array([1.0])\n",
    "\n",
    "        # stack them together\n",
    "        C_constraint_matrix = np.vstack([no_leverage_constraint_coefficient, no_short_selling_constraint_coefficient])\n",
    "        b_constraint_matrix = np.hstack([no_leverage_constraint_rhs, no_short_selling_constraint_rhs])\n",
    "\n",
    "        return C_constraint_matrix, b_constraint_matrix\n",
    "    \n",
    "        \n",
    "    def calculate_constrained_mv_optimisation(self):\n",
    "        implied_returns_vector_values = self.calculate_implied_equilibrium_returns().values\n",
    "\n",
    "        # we are solving an equation of the form - 1/2 x^T G x - a^T x s.t. C.T x >= b\n",
    "        quadratic_G = self.risk_aversion * self._covariance_matrix.values\n",
    "\n",
    "        quadratic_a = self.calculate_views_adjusted_returns().values.flatten()\n",
    "\n",
    "        # generate constraints\n",
    "        C_constraint_matrix, b_constraint_matrix = self.generate_mv_constraint_matrices(implied_returns_vector_values)\n",
    "\n",
    "        # solve for weights\n",
    "        optimal_weights = quadprog.solve_qp(quadratic_G, quadratic_a, C_constraint_matrix.T, b_constraint_matrix, meq=1)[0]\n",
    "\n",
    "        optimal_weights = pd.DataFrame(optimal_weights, columns=[\"Constrained Adjusted Weights\"], index=self._log_returns.columns)\n",
    "\n",
    "        return optimal_weights\n",
    "    \n",
    "    \n",
    "    def calculate_tracking_error_optimisation(self):\n",
    "        implied_returns_vector_values = self.calculate_implied_equilibrium_returns().values\n",
    "\n",
    "        number_of_assets = len(implied_returns_vector_values)\n",
    "\n",
    "        adjusted_returns = self.calculate_views_adjusted_returns().values\n",
    "\n",
    "        # constraints\n",
    "        constraints = [{'type': 'eq', 'fun': self.zero_sum_equality_constraint},\n",
    "                       {'type': 'eq', 'fun': self.tracking_error_target_constraint}]\n",
    "        \n",
    "\n",
    "        # bounds - no short selling or other constraints\n",
    "        bounds = [(None, None)] * number_of_assets\n",
    "\n",
    "        # initial guess - very small non-zero deviation\n",
    "        x0 = np.ones(number_of_assets) * 1e-3\n",
    "\n",
    "        # generate weights\n",
    "        results = minimize(fun= self.objective_function, x0=x0, bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        if not results.success:\n",
    "            print(\"Optimization failed:\", results.message)\n",
    "            return None\n",
    "\n",
    "        optimal_deviations = results.x\n",
    "\n",
    "        # create a series from the deviations with the same index as the eq weights\n",
    "        optimal_deviations = pd.Series(results.x, index=self._equilibrium_weights.index)\n",
    "\n",
    "        optimal_weights = self._equilibrium_weights + optimal_deviations\n",
    "\n",
    "        optimal_weights = pd.DataFrame(optimal_weights, columns=[\"TE Constrained Adjusted Weights\"], index=self._log_returns.columns)\n",
    "\n",
    "        return optimal_weights\n",
    "    \n",
    "    # Tracking Error - objective and constraints\n",
    "    def objective_function(self, x):\n",
    "        # maximise returns\n",
    "        adjusted_returns = self.calculate_views_adjusted_returns().values\n",
    "        value = - np.dot(x, adjusted_returns)\n",
    "        # print(f\"Objective function value: {value} for x: {x}\")\n",
    "\n",
    "\n",
    "        return value\n",
    "    \n",
    "    def zero_sum_equality_constraint(self, x):\n",
    "        # constraint - sum of deviations is zero\n",
    "        value = np.sum(x)\n",
    "        # print(f\"Zero sum constraint value: {value} for x: {x}\")\n",
    "\n",
    "        return value\n",
    "    \n",
    "    def tracking_error_target_constraint(self, x):\n",
    "        value = np.dot(x.T, np.dot(self.covariance_matrix, x)) - self._tracking_error_target\n",
    "        # print(f\"Tracking error constraint value: {value} for x: {x}\")\n",
    "\n",
    "        return value\n",
    "\n",
    "\n",
    "    # attributes\n",
    "    # tau\n",
    "    @property\n",
    "    def tau(self):\n",
    "        return self._tau\n",
    "\n",
    "    @tau.setter\n",
    "    def tau(self, value):\n",
    "        self._tau = value\n",
    "\n",
    "    # risk aversion \n",
    "    @property\n",
    "    def risk_aversion(self):\n",
    "        return self._risk_aversion\n",
    "\n",
    "    @risk_aversion.setter\n",
    "    def risk_aversion(self, value):\n",
    "        self._risk_aversion = value\n",
    "\n",
    "    @property\n",
    "    def covariance_matrix(self):\n",
    "        return self._covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the class\n",
    "equilibrium_weights = pd.Series({\n",
    "    'Energy': 0.037,\n",
    "    'Materials': 0.026,\n",
    "    'Industrials': 0.08,\n",
    "    'Consumer Discretionary': 0.118,\n",
    "    'Consumer Staples': 0.062,\n",
    "    'Health Care': 0.1330,\n",
    "    'Financials': 0.1150,\n",
    "    'Information Technology': 0.281,\n",
    "    'Communication Services': 0.096,\n",
    "    'Utilities': 0.026,\n",
    "    'Real Estate': 0.026\n",
    "})\n",
    "\n",
    "risk_aversion_dict = pd.Series({\n",
    "    # in order of increasing aversion\n",
    "    'Kelly': 0.01,\n",
    "    'Market': 2.24,\n",
    "    'Trustee': 6\n",
    "})\n",
    "\n",
    "# view adjusted returns with defined views\n",
    "k = 3\n",
    "n = 11\n",
    "\n",
    "#  these are annual views so we need to divide by 252 to make them daily\n",
    "Q = np.array([0.07, 0.0025, 0.01]).reshape(-1, 1) / 252\n",
    "\n",
    "P = np.zeros((k, n))\n",
    "\n",
    "# bl_model.calculate_views_adjusted_returns()\n",
    "# first view\n",
    "P[0, 0] = 1\n",
    "# second view\n",
    "P[1, 6] = -1\n",
    "P[1, 7] = 1\n",
    "# third view\n",
    "P[2, 3] = -0.24\n",
    "P[2, 4] = 0.71\n",
    "P[2, 7] = -0.76\n",
    "P[2, 9] = 0.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------ WRITE UP ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 What is the Black-Litterman Portfolio\n",
    "\n",
    "The initial formulation of the Black-Litterman model for Portfolio construction was described in the 1992 paper. The paper described an intuitive solution by combining the mean-variance opitimisation framework of Markowitz and the capital asset pricing model (CAPM) of Sharpe and Lintner. The issue with the CAPM model and minimum-variance portfolio decribed by Markowitz is that small tweaks in the values could lead to significant rebalancing of the portfolio. The Markowitz framework effectively takes the risk-free return as the alternative benchmark. The Black-Litterman model on the otherhand uses equilibrium risk premiums to provide a neutral reference point for expected returns, generating a market-capitalisation-weighted-portfolio and then incorporating subjective views that tilts in the direction of assets favored by the investor to enable the generation of alpha. The Black-Litterman models assumes does not assume the model is always at CAPM equilibrium, but any shift away from this equilibrium will experience pressure from the market to revert. Furthermore, the model allows the investor to have as many, or as few, views as they wish on either an absolute or relative basis. \n",
    "\n",
    "We will undertake the following steps in the next few sections to construct our portfolio according to the Black-Litterman framework. Firstly, we need to assign our portfolio weights based upon the market capitalisation of our benchmark portfolio. As previously outlined we will assign 92% of our portfolio to the S&P 500 sectors that we have proxied via iShares ETFs and will according allocate based on their weight in the S&P 500 in 2022. The remaining 8% of the portfolio will be equally split across our three exogenous factors, four Fama-French factors and custom momentum factor. Based on these weights we will derive the implied market returns for each of our factors. Due to this method of deriving the market implied returns the frame work does not recommend deviating from the market returns until you formulate views about the factor that cause them to deviate on either an absolute or relative basis from the implied returns. Once we have formulated our views on the market then we will combine our market implied returns and views via Bayes Theorem to construct a posterior returns distribution. \n",
    "\n",
    "We will then use our posterior returns distribution to calculate our views adjusted optimal weights for our portfolio. Finally, once we have completed this optimisation we will compare how our new portfolio performs vs the benchmark S&P from 2022 to 2024. \n",
    "\n",
    "While the Markowitz mean-variance optimisation is designed to use discrete returns, the Black-Litterman model is based on log return which we will use here. \n",
    "\n",
    "$$\n",
    "\n",
    "R_{i} = log(1 + \\frac{P_{i+1}}{P_i})\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 6 Default Weights from S&P 500 per GICs Sector (2022)\n",
    "\n",
    "| Sector                  | Weights  |\n",
    "| :---------------------: | :---:    |\n",
    "| Information Technology  | 28.10%   |\n",
    "| Financials              | 11.50%   |\n",
    "| Health Care             | 13.30%   |\n",
    "| Consumer Discretionary  | 11.8%    |\n",
    "| Communication           | 9.60%    |\n",
    "| Industrials             | 8.00%    |\n",
    "| Consumer Staples        | 6.20%    |\n",
    "| Energy                  | 3.70%    |\n",
    "| Utilities               | 2.60%    |\n",
    "| Real Estate             | 2.60%    |\n",
    "| Materials               | 2.60%    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.5 Prior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Reverse Optimisation & Prior Returns\n",
    "\n",
    "Now that we have the optimal weights according to our equilibrium returns we are going to undertake a reverse optimisation of the weights to arrive at the returns vector for our sectors that would generate these weights. To solve this reverse optimisation we need to set up our problem. \n",
    "\n",
    "$$\n",
    "arg \\min_{\\omega} \\omega'\\pi - \\lambda\\omega'\\Sigma\\omega\n",
    "$$\n",
    "\n",
    "- $\\omega$: a vector of our sector weights\n",
    "- $\\pi$: a vector of our equilibrium returns\n",
    "- $\\lambda$: a scalar factor for risk-aversion\n",
    "- $\\Sigma$: covariance matrix for our returns\n",
    "\n",
    "Initially we will solve an unconstrained mean-variance and then will introduce constraints for a more robust solution. We take the derivative with respect to $w$ to get the following equation: \n",
    "\n",
    "$$\n",
    "\\pi - 2 \\lambda \\Sigma w = 0\n",
    "$$\n",
    "\n",
    "We re-arrange the equation for both $\\pi$ and $w$ to get the implied equilibrium return from our market weights and optimal weights under our model:\n",
    "\n",
    "$$\n",
    "\\pi^{*} = 2 \\lambda \\Sigma w\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{*} = \\frac{1}{2 \\lambda} \\Sigma^{-1} \\pi\n",
    "$$\n",
    "\n",
    "We can now calculate the implied market returns from our model based upon the market weights provided by the corresponding ETF proxies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
